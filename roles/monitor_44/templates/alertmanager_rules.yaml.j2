#jinja2:variable_start_string:'[%' , variable_end_string:'%]'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: monitoring
    prometheus-operator-validated: "true"
  generation: 1
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 23.3.2
    chart: kube-prometheus-stack-23.3.2
    heritage: Helm
    release: prometheus
  name: prometheus-kube-prometheus-istio.rules
  namespace: monitoring
spec:
  groups:
  - name: service.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[5m])))/1000
      labels:
        quantile: "0.99"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_5m
    - expr: |
        histogram_quantile(0.95, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[5m])))/1000
      labels:
        quantile: "0.95"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_5m
    - expr: |
        histogram_quantile(0.5, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[5m])))/1000
      labels:
        quantile: "0.5"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_5m
    - expr: |
        histogram_quantile(0.99, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[1h])))/1000
      labels:
        quantile: "0.99"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_1h
    - expr: |
        histogram_quantile(0.95, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[1h])))/1000
      labels:
        quantile: "0.95"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_1h
    - expr: |
        histogram_quantile(0.5, sum by (destination_service_name,le,namespace) (rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination"}[1h])))/1000
      labels:
        quantile: "0.5"
      record: destination_service_name_quantile:istio_request_duration:histogram_quantile_1h
    - expr: |
        sum by (destination_service_name, namespace) (increase(istio_requests_total{source_workload!="unknown",reporter="destination",response_code=~"^5.."}[5m]))
        /
        sum by (destination_service_name, namespace) (increase(istio_requests_total{source_workload!="unknown",reporter="destination"}[5m])) * 100
      record: destination_service_name:error_rate:ratio_5m
    - expr: |
        sum by (destination_service_name, namespace) (increase(istio_requests_total{source_workload!="unknown",reporter="destination",response_code=~"^5.."}[1h]))
        /
        sum by (destination_service_name, namespace) (increase(istio_requests_total{source_workload!="unknown",reporter="destination"}[1h])) * 100
      record: destination_service_name:error_rate:ratio_1h
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="1000",response_code!~"5.."}[5m])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[5m])
          )
        )
      record: destination_service_name:latency_1s_slo_error_rate:ratio_rate5m
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="1000",response_code!~"5.."}[1h])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[1h])
          )
        )
      record: destination_service_name:latency_1s_slo_error_rate:ratio_rate1h
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="5000",response_code!~"5.."}[5m])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[5m])
          )
        )
      record: destination_service_name:latency_5s_slo_error_rate:ratio_rate5m
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="5000",response_code!~"5.."}[1h])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[1h])
          )
        )
      record: destination_service_name:latency_5s_slo_error_rate:ratio_rate1h
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="10000",response_code!~"5.."}[5m])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[5m])
          )
        )
      record: destination_service_name:latency_10s_slo_error_rate:ratio_rate5m
    - expr: |
        1 -
        (
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_bucket{source_workload!="unknown",reporter="destination",le="10000",response_code!~"5.."}[1h])
          )
        /
          sum by (destination_service_name,namespace) (
            rate(istio_request_duration_milliseconds_count{source_workload!="unknown",reporter="destination"}[1h])
          )
        )
      record: destination_service_name:latency_10s_slo_error_rate:ratio_rate1h
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: monitoring
    prometheus-operator-validated: "true"
  creationTimestamp: "2021-12-17T08:20:28Z"
  generation: 1
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 23.3.2
    chart: kube-prometheus-stack-23.3.2
    heritage: Helm
    release: prometheus
  name: prometheus-kube-prometheus-general.rules
  namespace: monitoring
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDownAlphaOrPreProd
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
        runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/targetdown
        summary: One or more targets are unreachable.
        grafana: "grafana link"
        wiki: "wiki link"
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up{namespace=~".*-preprod"}) BY (job, namespace, service)) > 10
      for: 10m
      labels:
        severity: warning
    - alert: TargetDown
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
        runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/targetdown
        summary: One or more targets are unreachable.
        grafana: "grafana link"
        wiki: "wiki link"
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up{namespace!~".*-alpha|.*-preprod"}) BY (job, namespace, service)) > 10
      for: 10m
      labels:
        severity: critical
    - alert: Watchdog
      annotations:
        description: |
          This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-watchdog
        summary: An alert that should always be firing to certify that Alertmanager
          is working properly.
      expr: vector(1)
      labels:
        severity: none
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: monitoring
    prometheus-operator-validated: "true"
  creationTimestamp: "2021-12-17T08:20:28Z"
  generation: 1
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 23.3.2
    chart: kube-prometheus-stack-23.3.2
    heritage: Helm
    release: prometheus
  name: prometheus-kube-prometheus-kubernetes-apps
  namespace: monitoring
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 10 minutes.
        runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepodcrashlooping
        summary: Pod is crash looping.
        grafana: "grafana link"
        wiki: "wiki link"
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace!~".*-alpha|.*-preprod"}[10m]) * 60 * 5 > 0
      for: 15m
      labels:
        severity: critical
    - alert: KubePodCrashLoopingAlphaOrPreProd
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 10 minutes.
        runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepodcrashlooping
        summary: Pod is crash looping.
        grafana: "grafana link"
        wiki: "wiki link"
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace=~".*-preprod"}[10m]) * 60 * 5 > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a
          non-ready state for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
        summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: |-
        sum by (namespace, pod) (
          max by(namespace, pod) (
            kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown"}
          ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
            1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
        summary: Deployment generation mismatch due to possible roll-back
      expr: |-
        kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
          has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
            >
          kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
        ) and (
          changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset
          }} has not matched the expected number of replicas for longer than 15
          minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but
          has not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |-
        kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset
          }} update has not been rolled out.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
        summary: StatefulSet update has not been rolled out.
      expr: |-
        (
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has
          not finished or progressed for at least 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
        summary: DaemonSet rollout is stuck.
      expr: |-
        (
          (
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          ) or (
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            0
          ) or (
            kube_daemonset_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          ) or (
            kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          )
        ) and (
          changes(kube_daemonset_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeContainerWaiting
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{
          $labels.container}} has been in waiting state for longer than 1 hour.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
        summary: Pod container waiting longer than 1 hour
      expr: sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",
        namespace=~".*"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
          $labels.daemonset }} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
        summary: DaemonSet pods are not scheduled.
      expr: |-
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
          $labels.daemonset }} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
        summary: DaemonSet pods are misscheduled.
      expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics",
        namespace=~".*"} > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeJobCompletion
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking
          more than 12 hours to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
        summary: Job did not complete in time
      expr: kube_job_spec_completions{job="kube-state-metrics", namespace=~".*"}
        - kube_job_status_succeeded{job="kube-state-metrics", namespace=~".*"}  >
        0
      for: 12h
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
          complete. Removing failed job after investigation should clear this alert.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
        summary: Job failed to complete.
      expr: kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaReplicasMismatch
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has not matched the desired number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
        summary: HPA has not matched descired number of replicas.
      expr: |-
        (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
          >
        kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~".*"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
          <
        kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"})
          and
        changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}[15m]) == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaMaxedOut
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has been running at max replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
        summary: HPA is running at max replicas
      expr: |-
        kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
          ==
        kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"}
      for: 15m
      labels:
        severity: warning